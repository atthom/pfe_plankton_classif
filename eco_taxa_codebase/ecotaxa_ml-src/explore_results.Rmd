---
title: "Automatic classification of plankton images"
output:
  html_document:
    theme: paper
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
runtime: shiny_prerendered
---

<style type = "text/css">
.tocify-subheader .tocify-subheader .tocify-item {
  font-style: italic;
  padding-left: 35px;
}
/*body {
  line-height: 1.6;
}*/
.table-condensed>tbody>tr>td, .table-condensed>tbody>tr>th, 
.table-condensed>thead>tr>td, .table-condensed>thead>tr>th, 
.table-condensed>tfoot>tr>td, .table-condensed>tfoot>tr>th {
  line-height: 1.3;
}
</style>

```{r setup, include=FALSE, context="setup"}
# load packages
library("plyr")
library("tidyverse")
library("printr")
library("stringr")
library("DT")
library("viridis")
library("shiny")
library("knitr")
library("codetools")
library("lubridate")
library("readxl")

opts_chunk$set(
  # output
  echo=FALSE,
  fig.width=10*90/72, fig_height=5*90/72, dpi=72,
  comment=NA,
  # cache
  cache=TRUE
)

# allow better formatting of tables
options(knitr.table.format = 'html')

theme_set(theme_gray(14) + theme(legend.position="top"))
```

## Data

```{r read_images_data, context="data"}
# read data
library("gsheet")
flowcam <- gsheet2tbl("https://docs.google.com/spreadsheets/d/1cFNKsP7lKGOI7bZsqz1SNLIeWHpeIpzyaZV_yBd_0dI")
flowcam$instrument <- "FlowCam"
uvp <- gsheet2tbl("https://docs.google.com/spreadsheets/d/1NLcVg4pWKYC4pcz-Bk2eGdmtCiU6cd-EG3yhrWRkay8")
uvp$instrument <- "UVP5"
zoocam <- gsheet2tbl("https://docs.google.com/spreadsheets/d/14Ut8uz_lm51tCO4huY6IU2WEd8oXJ6E8YONvUA2fJ7U")
zoocam$instrument <- "ZooCam"
zooscan <- gsheet2tbl("https://docs.google.com/spreadsheets/d/1QFlLGw7ENWeggl8Km4ANdEABETMx97_Rj6FgIBp-7BQ")
zooscan$instrument <- "ZooScan"
# combine and reformat data
taxo_groupings <- bind_rows(flowcam, uvp, zoocam, zooscan) %>%
  select(-ends_with("amanda"), -id, -name, -starts_with("comment")) %>% 
  gather(key="taxonomic_level", value="name", group1, group2)
```

```{r read_result_contents}
rc <- read_excel("../results/contents.xlsx") %>% 
  # keep only relevant models
  filter(!is.na(keep)) %>% select(-keep) %>% 
  # extract info about models
  mutate(fixedpath=ifelse(str_count(path, "_")==3, str_replace(path, "_2017", "__2017"), path)) %>% 
  separate(fixedpath, into=c("model", "instrument", "taxonomic_level", "split", "date"), sep="_") %>% 
  # make data extraction easier
  mutate(
    path=str_c("../results/", path),
    date=parse_date_time(date, orders="YmdHM")
  )
```


### Description

Identified images are assembled from EcoTaxa. Examples of the identified images (50 at most per class) are available for browsing for each instrument: [FlowCam](http://ecotaxa.obs-vlfr.fr/prj/607), [UVP5](http://ecotaxa.obs-vlfr.fr/prj/606), [ZooCam](http://ecotaxa.obs-vlfr.fr/prj/605), [ZooScan](http://ecotaxa.obs-vlfr.fr/prj/604).

From this detailed classification information, two taxonomic levels are considered: `group1` is the finest we can hope to train on and `group2` is a coarser level, in which some classes are grouped together to provide more data per class. Not all identified images are retained in these groupings. Some classes are discarded because they are:

1. either very rare classes that are not easily regrouped in other classes but cannot be trained on individually;
2. or super-classes, in which images were stored but never sorted further and for which only the more detailed underlying classes are kept for training (e.g. `Copepoda`, which is discarded to keep only the detailed classes of copepods).

The data is then split into training (80%) and test (20%) sets. The test split is not used here and will only be used at the very end. The training data is further split into 3 cross-validation folds (0, 1, 2). Each model is trained on two folds (e.g. 0, 1) and validated on the remaining one (e.g. 2). For each model, two errors are therefore available: the training error (computed on a self-prediction of the two training folds; which usually under-estimates the actual error) and the validation error (computed on the prediction of the validation fold; which should approach the test error). All splits are stratified per classes in `group1`, to be sure to get enough data in each class of each split. 

The number of classes and number of images in the training set for each instrument are

```{r summarise_images_data}
stats <- taxo_groupings %>%
  group_by(instrument, taxonomic_level) %>% filter(!is.na(name)) %>% summarise(
    n_images=sum(n, na.rm=T),
    n_classes=length(unique(name))
  ) %>% ungroup() %>% 
  gather(key="var", val="val", n_classes, n_images) %>% 
  unite(col="var", taxonomic_level, var, sep=":") %>% 
  spread(var, val)
knitr::kable(stats, format="html", table.attr = "class='table table-condensed table-striped'", format.args=list(big.mark=","))
```

The numbers of images can be higher for `group2` because some super-classes not considered in `group1` are considered in `group2` (e.g. for ZooScan, images classified as `Harosa` are not considered in `group1` because only finer subdivisions are; they are considered and regrouped with their children in `group2`).


### Number per class

The number of images per class are

```{r summarise_taxo_ui}
inputPanel(
  selectInput("t_instrument", label="Instrument", choices=unique(taxo_groupings$instrument), selected=),
  selectInput("t_taxonomic_level", label="Taxonomic level", choices=unique(taxo_groupings$taxonomic_level))
)
dataTableOutput("tbl_summarise_taxo")
```

```{r summarise_taxo_server, cache=FALSE, context="server"}
output$tbl_summarise_taxo <- renderDataTable({
  dat <- taxo_groupings %>% filter(instrument == input$t_instrument, taxonomic_level == input$t_taxonomic_level)
  counts <- dat %>% group_by(name) %>% summarise(n=sum(n, na.rm=T)) %>% as.data.frame()
  counts$name <- factor(counts$name, levels=unique(dat$name))
  counts <- arrange(counts, name)
  counts$name <- as.character(counts$name)
  datatable(counts, options=list(searching=FALSE, paging=FALSE, info=FALSE), rownames=T, style="bootstrap", class="table-condensed table-striped", width="50%")
})
```

NB:

- `NA` correspond to the discarded classes. The numbers can be significant because super-classes can be large. Those are not counted in the previous table.
- Some classes have very low numbers and should have been removed...


## ZooProcess + Random Forest

Images are classified using pre-computed features (by ZooProcess for all instruments but the ZooCam) in a Random Forest (RF) classifier. The RF implementation in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) is used.

Images in each class are weighted by `1/class_effective`, effectively giving equal weight to each class in the classification task. This avoids any bias by the training set and increases the generality of the model; but it may diminish performance by not using some obvious prior information (such as the fact that Copepods and detritus are more frequent than the rest in any plankton dataset).

### Grid search for parameters

Various combinations of parameters are tested to find the optimal ones. The recommended text-book values are:

- `max_features` = sqrt(number of actual features) ~ 7 for most datasets
- `min_samples_leaf` = 5 for a classification task
- `n_estimators` = as large as possible

The accurary estimates of various models can be explored below.

```{r rf_grid_read, context="data"}
r <- adply(filter(rc, model=="RF"), 1, function(x) {
  r <- read_csv(str_c(x$path, "/results.csv"), col_types=cols(fold=col_skip()))
  return(r)
})
# reformat and keep only scores
r <- r %>% select(-starts_with("time")) %>% gather(key="dataset", val="accuracy", starts_with("score")) %>% arrange(max_features, min_samples_leaf, n_estimators)
r$dataset <- str_replace(r$dataset, "score_", "")
```

```{r rf_grid_ui, warning=FALSE}
# prepare plot options
inputPanel(
  checkboxGroupInput("rf_instrument", label="Instrument", choices=unique(r$instrument), selected=unique(r$instrument)),
  checkboxGroupInput("rf_taxonomic_level", label="Taxonomic level", choices=unique(r$taxonomic_level), selected=unique(r$taxonomic_level)),
  checkboxGroupInput("rf_dataset", label="Dataset", choices=unique(r$dataset), selected=unique(r$dataset)),
  checkboxGroupInput("rf_max_features", label="Max nb features", choices=unique(r$max_features), selected=unique(r$max_features)),
  checkboxGroupInput("rf_min_samples_leaf", label="Min nb samples/leaf", choices=unique(r$min_samples_leaf), selected=unique(r$min_samples_leaf)),
  checkboxGroupInput("rf_n_estimators", label="Nb estimators (trees)", choices=unique(r$n_estimators), selected=unique(r$n_estimators)),
  selectInput("rf_colour", "Colour by", choices=c("none", "instrument", "group1/group2"="taxonomic_level", "train/val"="dataset"), selected="instrument"),
  selectInput("rf_subplot", "Subplot by", choices=c("none"=".", "instrument", "group1/group2"="taxonomic_level", "train/val"="dataset"), selected="dataset")
)
textOutput("rf_grid_text")
suppressWarnings(plotOutput("rf_grid_plot", height="auto"))
```

```{r rf_grid_server, cache=FALSE, context="server", warning=FALSE}
# pre-select data
filter_data <- reactive({
  filter(r,
    instrument %in% input$rf_instrument,
    taxonomic_level %in% input$rf_taxonomic_level,
    dataset %in% input$rf_dataset,
    max_features %in% input$rf_max_features,
    min_samples_leaf %in% input$rf_min_samples_leaf,
    n_estimators %in% input$rf_n_estimators
  )
})

# show number of models
output$rf_grid_text <- renderText({
  str_c(nrow(filter_data()), " models retained.")
})

plotHeight <- reactive({
  if (input$rf_subplot == ".") {
    350
  } else {
    length(unique(filter_data()[,input$rf_subplot]))*300
  }
})

# plot data
output$rf_grid_plot <- renderPlot({
  d <- filter_data()
  if(input$rf_colour == "none") { colour<- NULL }  else { colour <-  input$rf_colour }
  dd <- d %>%
    gather(key="param", value="param_val", n_estimators, min_samples_leaf, max_features)
  p <- ggplot(dd, aes_string(x="param_val", y="accuracy", colour=colour, fill=colour)) + geom_smooth(method="loess", size=0.5, span=0.8) + geom_point(alpha=0.3, size=1.5, shape=16) + facet_grid(str_c(input$rf_subplot, "~param"), scales="free")
  suppressWarnings(print(p))
}, width=800, height=plotHeight)
```

Overall, the most influential parameter is `min_samples_leaf` which should be 2. Increasing `max_features` helps a little so setting it to 11 is best. `n_estimators`=100 is already quite good, 400 improves very little, if at all, compared to 300. 200 is probably a good compromise between training speed and accuracy. 

In more details, accuracy is better for zoocam > uvp5 > flowcam. Accuracy for `group1` and `group2` are comparable, except for ZooCam where `group2` (coarser groups) yields better accuracy. This is probably because the groupings are much coarser than in the other data sets (all Copepoda, all Rhizaria, etc.). This suggests that groupings for the other datasets are not coarse enough.

### Confusion matrix and stats

With `min_samples_leaf`=2, `max_features`=11, and `n_estimators`=300, we compute the confusion matrices averaged over the three cross-validation folds.

```{r rf_cm_read, context="data"}
# get appropriate model IDS
best_rf_models <- filter(r, min_samples_leaf==2, max_features==11, n_estimators==300)

cm_rf <- dlply(best_rf_models, ~instrument+taxonomic_level, function(x) {
  # read CM
  # NB: rows are true group, cols are predicted groups
  cm <- laply(str_c(x$path, "/confusion_matrix_", x$model_id, ".csv")[1], function(f) {as.matrix(read.table(f))})
  # sum over the three folds
  # cm <- aaply(cm, 2:3, sum)
  
  cm <- t(cm)
  # NB: this is strange compared to the definition in sklearn. check
  
  # read and assign the classes names
  classes <- scan(str_c("../results/classes/", x$instrument[1], "_classes_", x$taxonomic_level[1], ".txt"), what="character", quiet=T)
  colnames(cm) <- classes
  rownames(cm) <- classes
  
  # discard very rare classes
  # idx <- rowSums(cm) > 1
  # cm <- cm[idx,idx]

  return(cm)
})

# cm <- cm_rf[[1]]
# from_cm <- data.frame(prop_cm=round(rowSums(cm) / sum(rowSums(cm)) * 100, 5))
# from_cm$name <- row.names(from_cm)
# real <- i %>% filter(instrument == "FlowCam", taxonomic_level=="group1") %>% filter(!is.na(name)) %>% group_by(name) %>% summarise(n=sum(n, na.rm=T)) %>% mutate(prop=round(n/sum(n, na.rm=T)*100,5)) %>% select(name, prop) %>% arrange(name)
# d <- left_join(real, from_cm)
# View(d)

```

```{r rf_cm_ui}
inputPanel(
  selectInput("cm_rf_instrument", label="Instrument", choices=unique(best_rf_models$instrument)),
  selectInput("cm_rf_taxonomic_level", label="Taxonomic level", choices=unique(best_rf_models$taxonomic_level)),
  selectInput("cm_rf_scaling", label="Color scaling", choices=c("none", "log(n+1)"="log", "row (recall)"="row", "col (precision)"="col"), selected="log")
)

h4("Matrix")
plotOutput("rf_cm_plot", height="auto")

h4("Stats")
dataTableOutput("rf_cm_table")
```

```{r cm_functions, context="data"}
cm_heat <- function(cm, scale) {
  library("viridis")
  if (scale == "log") {
    cm <- log1p(cm)
    scale <- "none"
  }
  
  heatmap(cm,
    scale=scale,
    Rowv=NA, Colv=NA, revC=TRUE,
    col=viridis(50),
    xlab="Pred", ylab="True",
    margins=c(10,10),
    cexRow=0.9, cexCol=0.9
  )
}

cm_stats <- function(cm) {
  library("dplyr")
  library("DT")
  
  stats <- data.frame(
    class=rownames(cm),
    recall=diag(cm)/rowSums(cm),
    precision=diag(cm)/colSums(cm)
  ) %>%
  mutate(
    F1=2*precision*recall/(precision+recall)
  )
  datatable(stats, options=list(searching=FALSE, paging=FALSE, info=FALSE), rownames=F, style="bootstrap", class="table-condensed table-striped") %>% 
    formatPercentage(c("recall", "precision"), digits=1) %>% 
    formatRound("F1", digits=2)
}
```


```{r rf_cm_server, cache=F, context="server"}
ccm_rf <- reactive({
  cm_rf[[str_c(input$cm_rf_instrument, ".", input$cm_rf_taxonomic_level)]]
})

output$rf_cm_plot <- renderPlot({
  cm_heat(ccm_rf(), input$cm_rf_scaling)
}, width=800, height=800)

output$rf_cm_table <- renderDataTable({
  cm_stats(ccm_rf())
})
```


## SparseConvNet

SparseConvNet models are trained with the following parameters:

- images down-scaled based on their diagonal size (to fit in the memory of the GPU more readily) based on a logistic curve (see below). Until 150px in diagonal they are not rescaled and then are progressively rescaled to a maximum diagonal of 300px. 90% of the images have a diagonal smaller than 30, 253, 310, and 377 px respectively for uvp5, flowcam, zooscan, and zoocam, so rescaling this way should almost never completely loose the information about relative size.

```{r scn_img_scaling}
# logistic scaling
diagonal <- 0:750
maxDiagonalSize <- 300
linDiagonalSize <- 0.5 * maxDiagonalSize
k <- 4 / maxDiagonalSize
scaled_diagonal <- ifelse(diagonal > linDiagonalSize, maxDiagonalSize / (1 + exp(-k * (diagonal - linDiagonalSize))), diagonal);
qplot(diagonal, scaled_diagonal, geom="path") + geom_vline(aes(xintercept=x, colour=instrument), data=data.frame(x=c(30, 253, 310, 377), instrument=c("UVP5", "FlowCam", "ZooScan", "ZooCam"))) + labs(colour="90% percentile for:")
```

- data augmentation at training (scale, rotate, shear, translate) and testing (rotate, translate) times
- `exemplarsPerClassPerEpoch`=1000 yielding a model with no prior bias for any class (which is what we want to be able to extract features relevant for all classes)
- Fractional Max Pooling with `sqrt(2)` stride
- 12 layers (therefore yielding a 2292Ã—2292 px input field)
- `initialLearningRate`=0.003 and `learningRateDecay`=0.05
- `dropoutMultiplier`=0
- 100 epochs


### Training log

We first explore how the percentage of `mistakes` (100-accuracy) and the negative log-likelihood (`nll`) evolve during training.

```{r scn_train_read, context="data"}
l <- adply(filter(rc, model=="SCN"), 1, function(x) {
  d <- read_csv(str_c(x$path, "/wp2.csv"), col_types=cols())
  # keep only the relevant test errors (last one)
  d <- d %>%
    select(epoch, starts_with("train"), starts_with(str_c("test_3")))
  # rename columns
  names(d) <- str_replace(names(d), "test_3", "val")
  return(d)
})

l <- l %>% gather(key="var", value="val", starts_with("train"), starts_with("val")) %>%
  separate(col=var, into=c("dataset", "diagnostic"), sep="_")
```

```{r scn_train_ui}
# prepare plot options
inputPanel(
  checkboxGroupInput("scn_instrument", label="Instrument", choices=unique(l$instrument), selected=unique(l$instrument)),
  checkboxGroupInput("scn_taxonomic_level", label="Taxonomic level", choices=unique(l$taxonomic_level), selected=unique(l$taxonomic_level)),
  checkboxGroupInput("scn_dataset", label="Dataset", choices=unique(l$dataset), selected=unique(l$dataset)),
  selectInput("scn_colour", "Colour by", choices=c("none", "instrument", "group1/group2"="taxonomic_level", "train/val"="dataset"), selected="instrument"),
  selectInput("scn_subplot", "Subplot by", choices=c("none"=".", "instrument", "group1/group2"="taxonomic_level", "train/val"="dataset"), selected="dataset"),
  sliderInput("scn_epoch", "Epochs", min=min(l$epoch), max=max(l$epoch), value=range(l$epoch))
)
plotOutput("scn_train_plot", height="auto")
```

```{r scn_train_server, cache=FALSE, context="server"}
# plot data
output$scn_train_plot <- renderPlot({
  d <- l %>% filter(
      instrument %in% input$scn_instrument,
      taxonomic_level %in% input$scn_taxonomic_level,
      dataset %in% input$scn_dataset,
      epoch >= input$scn_epoch[1], epoch <= input$scn_epoch[2]
    )
  if (input$scn_subplot != ".") {
    d$facet <- interaction(d$diagnostic, d[,input$scn_subplot])
  } else {
    d$facet <- d$diagnostic
  }
  if(input$scn_colour == "none") { colour<- NULL }  else { colour <-  input$scn_colour }
  d %>%
    ggplot(aes_string(x="epoch", y="val", colour=colour, fill=colour, group="path")) + geom_path(size=0.5) + facet_wrap(~facet, ncol=2, scales="free_y")
}, width=800, height=600)
```

Training decreases the error until a stable state. We would expect the validation error to go back up at some point, denoting overfitting, and it does not, meaning we should train longer or increase the learning rate. As before, the error is quite similar for `group1` and `group2` and accuracy is: flowcam (~65%) > uvp5 (~50%).

### Confusion matrix and stats

The per-class confusion statistics for the predictions at epoch=`r max(l$epoch)` can be explored below.

```{r scn_cm_read, context="data"}
scn_models <- filter(rc, model=="SCN")

cm_scn <- dlply(scn_models, ~instrument+taxonomic_level, function(x) {
  # read CF
  # NB: rows are true group, cols are predicted groups
  cm <- laply(str_c(x$path, "/_validation_confusion.csv")[1], function(f) {as.matrix(read.table(f))})
  # sum over the three folds
  # cm <- aaply(cm, 2:3, sum)
  
  # read and assign the classes names
  classes <- scan(str_c("../results/classes/", x$instrument[1], "_classes_", x$taxonomic_level[1], ".txt"), what="character", quiet=T)
  colnames(cm) <- classes
  rownames(cm) <- classes
  
  # discard very rare classes
  # idx <- rowSums(cm) > 1
  # cm <- cm[idx,idx]

  return(cm)
})

# from_cm <- data.frame(prop_cm=round(rowSums(cm) / sum(rowSums(cm)) * 100, 5))
# from_cm$name <- row.names(from_cm)
# real <- i %>% filter(instrument == "FlowCam", taxonomic_level=="group1") %>% filter(!is.na(name)) %>% group_by(name) %>% summarise(n=sum(n, na.rm=T)) %>% mutate(prop=round(n/sum(n, na.rm=T)*100,5)) %>% select(name, prop) %>% arrange(name)
# d <- left_join(real, from_cm)
# View(d)
```

```{r scn_cm_ui}
inputPanel(
  selectInput("cm_scn_instrument", label="Instrument", choices=unique(scn_models$instrument)),
  selectInput("cm_scn_taxonomic_level", label="Taxonomic level", choices=unique(scn_models$taxonomic_level)),
  selectInput("cm_scn_scaling", label="Color scaling", choices=c("none", "log(n+1)"="log", "row (recall)"="row", "col (precision)"="col"), selected="log")
)

h4("Matrix")
plotOutput("scn_cm_plot", height="auto")

h4("Stats")
dataTableOutput("scn_cm_table")
```

```{r scn_cm_server, cache=F, context="server"}
ccm_scn <- reactive({
  cm_scn[[str_c(input$cm_scn_instrument, ".", input$cm_scn_taxonomic_level)]]
})

output$scn_cm_plot <- renderPlot({
  cm_heat(ccm_scn(), input$cm_scn_scaling)
}, width=800, height=800)

output$scn_cm_table <- renderDataTable({
  cm_stats(ccm_scn())
})
```
